{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution of Logistic Regression:  [ 1.53222656 -4.0838038 ]\n",
      "Final loss:  0.40186247596433916\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(S):\n",
    " \"\"\"\n",
    " S: an numpy array\n",
    " return sigmoid function of each element of S\n",
    " \"\"\"\n",
    " return 1/(1 + np.exp(-S))\n",
    "\n",
    "def prob(w, X):\n",
    " \"\"\"\n",
    " X: a 2d numpy array of shape (N, d). N datatpoint, each with size d\n",
    " w: a 1d numpy array of shape (d)\n",
    " \"\"\"\n",
    " return sigmoid(X.dot(w))\n",
    "\n",
    "def loss(w, X, y, lam):\n",
    " \"\"\"\n",
    " X, w as in prob\n",
    " y: a 1d numpy array of shape (N). Each elem = 0 or 1\n",
    " \"\"\"\n",
    " z = prob(w, X)\n",
    " return -np.mean(y*np.log(z) + (1-y)*np.log(1-z)) + 0.5*lam/X.shape[0]*np.sum(w*w)\n",
    "\n",
    "def logistic_regression(w_init, X, y, lam = 0.001, lr = 0.1, nepoches = 2000):\n",
    " # lam - reg paramether, lr - learning rate, nepoches - number of epoches\n",
    " N, d = X.shape[0], X.shape[1]\n",
    " w = w_old = w_init\n",
    " loss_hist = [loss(w_init, X, y, lam)] # store history of loss in loss_hist\n",
    " ep = 0\n",
    " while ep < nepoches:\n",
    "  ep += 1\n",
    "  mix_ids = np.random.permutation(N)\n",
    "  for i in mix_ids:\n",
    "   xi = X[i]\n",
    "   yi = y[i]\n",
    "   zi = sigmoid(xi.dot(w))\n",
    "   w = w - lr*((zi - yi)*xi + lam*w)\n",
    "  loss_hist.append(loss(w, X, y, lam))\n",
    "  if np.linalg.norm(w - w_old)/d < 1e-6:\n",
    "   break\n",
    "  w_old = w\n",
    " return w, loss_hist\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, \n",
    "2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50]]).T\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "# bias trick\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1)\n",
    "w_init = np.random.randn(Xbar.shape[1])\n",
    "lam = 0.0001\n",
    "w, loss_hist = logistic_regression(w_init, Xbar, y, lam, lr = 0.05, nepoches = 500)\n",
    "print('Solution of Logistic Regression: ',w)\n",
    "print('Final loss: ',loss(w, Xbar, y, lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
