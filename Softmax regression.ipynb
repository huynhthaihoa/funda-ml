{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(Z):\n",
    " \"\"\"\n",
    " Compute softmax values for each sets of scores in V.\n",
    " each column of V is a set of scores.\n",
    " Z: a numpy array of shape (N, C)\n",
    " return a numpy array of shape (N, C)\n",
    " \"\"\"\n",
    " e_Z = np.exp(Z)\n",
    " A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    " return A\n",
    "\n",
    "def softmax_stable(Z):\n",
    " \"\"\"\n",
    " Compute softmax values for each sets of scores in Z.\n",
    " each row of Z is a set of scores.\n",
    " \"\"\"\n",
    " # Z = Z.reshape(Z.shape[0], -1)\n",
    " e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))\n",
    " A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    " return A\n",
    "\n",
    "def softmax_loss(X, y, W):\n",
    " \"\"\"\n",
    " W: 2d numpy array of shape (d, C),\n",
    " each column correspoding to one output node\n",
    " X: 2d numpy array of shape (N, d), each row is one data point\n",
    " y: 1d numpy array -- label of each row of X\n",
    " \"\"\"\n",
    " A = softmax_stable(X.dot(W))\n",
    " id0 = range(X.shape[0]) # indexes in axis 0, indexes in axis 1 are in y\n",
    " return -np.mean(np.log(A[id0, y]))\n",
    "\n",
    "def softmax_grad(X, y, W):\n",
    " \"\"\"\n",
    " W: 2d numpy array of shape (d, C),\n",
    " each column correspoding to one output node\n",
    " X: 2d numpy array of shape (N, d), each row is one data point\n",
    " y: 1d numpy array -- label of each row of X\n",
    " \"\"\"\n",
    " A = softmax_stable(X.dot(W)) # shape of (N, C)\n",
    " id0 = range(X.shape[0])\n",
    " A[id0, y] -= 1 # A - Y, shape of (N, C)\n",
    " return X.T.dot(A)/X.shape[0]\n",
    "\n",
    "def softmax_fit(X, y, W, lr = 0.01, nepoches = 100, tol = 1e-5, batch_size = 10):\n",
    " W_old = W.copy()\n",
    " ep = 0\n",
    " loss_hist = [softmax_loss(X, y, W)] # store history of loss\n",
    " N = X.shape[0]\n",
    " nbatches = int(np.ceil(float(N)/batch_size))\n",
    " while ep < nepoches:\n",
    "  ep += 1\n",
    "  mix_ids = np.random.permutation(N) # mix data\n",
    "  for i in range(nbatches):\n",
    "   # get the i-th batch\n",
    "   batch_ids = mix_ids[batch_size*i:min(batch_size*(i+1), N)]\n",
    "   X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "   W -= lr*softmax_grad(X_batch, y_batch, W) # update gradient descent\n",
    "  loss_hist.append(softmax_loss(X, y, W))\n",
    "  if np.linalg.norm(W - W_old)/W.size < tol:\n",
    "   break\n",
    "  W_old = W.copy()\n",
    " return W, loss_hist\n",
    "\n",
    "def pred(W, X):\n",
    " \"\"\"\n",
    " predict output of each columns of X . Class of each x_i is determined by\n",
    " location of max probability. Note that classes are indexed from 0.\n",
    " \"\"\"\n",
    " return np.argmax(X.dot(W), axis =1)\n",
    "\n",
    "C, N = 5, 500 # number of classes and number of points per class\n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "X4 = np.random.multivariate_normal(means[4], cov, N)\n",
    "X = np.concatenate((X0, X1, X2, X3, X4), axis = 0) # each row is a datapoint\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1) # bias trick\n",
    "y = np.asarray([0]*N + [1]*N + [2]*N+ [3]*N + [4]*N)\n",
    "W_init = np.random.randn(Xbar.shape[1], C)\n",
    "W, loss_hist = softmax_fit(Xbar, y, W_init, batch_size = 10, nepoches = 100, lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "mnist = fetch_mldata('MNIST original', data_home='../../data/')\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)\n",
    "model = LogisticRegression(C = 1e5,\n",
    "solver = 'lbfgs', multi_class = 'multinomial') # C is inverse of lam\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy %.2f %%\" % (100*accuracy_score(y_test, y_pred.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
